{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Automatic_music_genaration.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"J4kPU-T4eV_h"},"source":["<center><img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"360\" height=\"160\" /></center>\n","\n","**<center><h1>Automatic music genaration using RNN</center>**"]},{"cell_type":"markdown","metadata":{"id":"ITNBPddZZsjL"},"source":["----\n","# **Table of Contents**\n","----\n","**1.** [**Problem Statement**](#Section1)<br>\n","**2.** [**Importing Libraries**](#Section2)<br>\n","**3.** [**Loading the data**](#Section3)<br>\n","**4.** [**Data-Preprocessing**](#Section4)<br>\n","**5.** [**Batch Genaration**](#Section5)<br>\n","**6.** [**Modelling**](#Section6)<br>\n","**7.** [**Music Genaration**](#Section7)<br>\n","**8.** [**Applications**](#Section8)<br>\n","**9.** [**Limitations**](#Section9)<br>\n","**10.** [**Conclusion**](#Section10)<br>"]},{"cell_type":"markdown","metadata":{"id":"7Cf-sAGcwo1e"},"source":["----\n","<a id=Section1></a>\n","# **1. Problem Statement**\n","----\n","- **ABC music prod. pvt.ltd** is a reknowned **audio-video production house** based out of **Mumbai, India**\n","\n","- As **COVID-19** cases are increasing day by day it is almost impossible for the musicians to cope up with real time studio work.\n","\n","- Hence, the company wants you to make an **AI based music genaration system.**\n","\n","- The goal of this project is to make an **AI based music genaration system.** \n","\n","- The key contraint to the problem is **accuracy.**\n","\n","\n","<center><img src = \"https://cdn.dribbble.com/users/316072/screenshots/10724786/laptop_music_animation_01_1600x1200.gif\"></center>\n","\n","### **Scenario**\n","\n","- You have been hired as a **freelance data scientist** for **ABC music prod. pvt.ltd**\n","\n","- The model should read a text file in **abc format**.\n"," \n","- The model should genarate the **corresponding music** framed out of that note sequence.\n"]},{"cell_type":"markdown","metadata":{"id":"GOztPJKBwqx1"},"source":["----\n","<a id=Section2></a>\n","# **2. Installing libraries**\n","----"]},{"cell_type":"code","metadata":{"id":"K2SqGIKKAGOC"},"source":["import numpy as np                                     # Importing numpy   \n","from keras.models import Sequential, load_model        # Importing Sequential layers and load_model \n","from keras.layers import LSTM, Dropout                 # Importing LSTM and Dropout layers  \n","from keras.layers import TimeDistributed               # Importing TimeDistributed layers\n","from keras.layers import Dense, Activation, Embedding  # Importing Dense, Activation and Embedding layers  \n","import os                                              # Importing OS \n","import json                                            # Importing JSON  \n","import argparse                                        # Importing Argsparse "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gq7wXmHmsgZI"},"source":["----\n","<a id=section3></a>\n","# **3. Data Reading**\n","----\n","- In this step we will **read the data** from the **input text corpus**.\n","- We will use **UTF-8** encoding for making the data"]},{"cell_type":"code","metadata":{"id":"Zeu80T0YaPGT"},"source":["with open('/content/input.txt','rb') as f:\n","      input_text = f.read()\n","input_text=str(input_text,'utf-8')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"LIm6agC7eWuN","outputId":"742fd54e-c279-4e8d-93ee-e613c7dcac1b"},"source":["input_text [:100]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'X: 1\\nT:A and D\\n% Nottingham Music Database\\nS:EF\\nY:AB\\nM:4/4\\nK:A\\nM:6/8\\nP:A\\nf|\"A\"ecc c2f|\"A\"ecc c2f|\"A\"'"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"4Vs-iPo6eV_t"},"source":["----\n","<a id=section4></a>\n","# **4. Data Preprocessing**\n","----\n","\n","- In this step we will make the data into the desired format.\n","- We will be converting each and every character into a integer.\n","- Then we will create a dictionary of it."]},{"cell_type":"code","metadata":{"id":"ZqW5Mn4HiIlz"},"source":["def generate_keys(text):\n","    #Charecter to index dictionary  \n","    char_to_idx = {ch:idx for idx,ch in enumerate(sorted(list(set(text))))}\n","\n","    #Index to character dicrionary\n","    idx_to_char = {idx:ch for ch,idx in char_to_idx.items()}\n","\n","    #Printing the charecter lengths\n","    print(\"length of the  charecter to index \",len(char_to_idx))\n","    print(\"length of the  index to charecter \",len(idx_to_char))\n","    return char_to_idx,idx_to_char"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zLCUHyqtl0Vf","outputId":"9fa7adb5-c691-47d6-b6f4-1ebcf93f4014"},"source":["# Genarating the charecter to index and index to charecter values\n","char_to_idx, idx_to_char = generate_keys(input_text)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["len of the  char_to_idx  86\n","len of the  idx_to_char  86\n"]}]},{"cell_type":"code","metadata":{"id":"RV2uFXgRnLYc"},"source":["# Reading the input text corpus\n","with open('/content/input.txt','w') as f:\n","      json.dump(char_to_idx,f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yMo-rh-_tD7L","outputId":"a917b2aa-18e0-42b3-8459-de22c0747fd3"},"source":["len(input_text)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["129665"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"zih4sOMCs1WR"},"source":["----\n","<a id=section5></a>\n","# **5. Batch genaration**\n","----\n","\n","- In this section we will be genarating batches in order to train our model.\n","- We can see that the **length is 129,665** \n","- We will be using a **batch size of 16** and a **sequence length of 64**\n","- Hence, the **number of batches** are going to be **((129,665/16)/ 64)** = 126\n","- Now since it is a sequence data we will divide the 1st **8104 char in 126 batches** , each batches will have these char in the 1st row.\n","- Similarly from **8105 - 16209th** chars will be divided into 126 batches (each batch will have 64 sequence) and will be added at the 2nd row of each batches\n","- This is how at **8104th interval** we will take char and divide them into batches and put them in respective rows of batches\n","- So we will have the **row wise continuation of the sequence** for different batches.  \n","- In this way we can keep the sequence information in the text data."]},{"cell_type":"code","metadata":{"id":"5Us8UJElqvxh"},"source":["# Here we are genarating respective batches to fit into the model\n","def generate_batchs(T, vocab_size):  \n","    length = T.shape[0] # length = 129665\n","    batch_char = int(length / batch_size); \n","    for start in range(0, 126*64,64):\n","        X = np.zeros((batch_size, batch_sequence)) \n","        Y = np.zeros((batch_size, batch_sequence, vocab_size)) \n","\n","        # Getting each batch index and column index\n","        for batch_index in range(0,batch_size):\n","            for col_index in range(0,batch_sequence):\n","              X[batch_index, col_index] = T[batch_char * batch_index + start + col_index]\n","              Y[batch_index, col_index, T[batch_char * batch_index + start + col_index+1]] = 1\n","        yield X,Y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SAV_KWg3v6Ws","outputId":"edb255e4-d185-47f7-9dfd-8f3702f69edf"},"source":["# Checking the size of the vocabulary\n","vocab_size = len(char_to_idx)\n","vocab_size"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["86"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"_O4H5wvHeV_1"},"source":["----\n","<a id=section6></a>\n","# **6. Modelling**\n","----\n","- In this step we will be making the **deep learning model**.\n","- We will train the model for a **100 epochs.**\n","- After every **10** epochs we will save the respective weights into the **main directory**."]},{"cell_type":"code","metadata":{"id":"IqGm2fO9A5JC"},"source":["# Defining a batch size of 16 and sequence length of 64\n","batch_size = 16\n","seq_len = 64\n","batch_sequence = 64\n","\n","# Getting the model directory\n","MODEL_DIR = '/content/'\n","\n","# Defining a function for saving weights\n","def save_weights(epoch, model):\n","    if not os.path.exists(MODEL_DIR):\n","        os.makedirs(MODEL_DIR)\n","    model.save_weights(os.path.join(MODEL_DIR, 'weights.{}.h5'.format(epoch)))\n","\n","# Defining a function for loading weights\n","def load_weights(epoch, model):\n","    model.load_weights(os.path.join(MODEL_DIR, 'weights.{}.h5'.format(epoch)))\n","\n","# Defing a function for model development\n","def build_model(batch_size, seq_len, vocab_size):\n","    model = Sequential()\n","    model.add(Embedding(vocab_size,\n","                        512, \n","                        batch_input_shape=(batch_size, seq_len)))\n","    \n","    for i in range(3):\n","        model.add(LSTM(256, return_sequences=True, stateful=True))\n","        model.add(Dropout(0.2))\n","    ## Using Time Distributed Dense Layer for each return sequences\n","    model.add(TimeDistributed(Dense(vocab_size))) \n","    model.add(Activation('softmax'))\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KKCrgbc7BSK0"},"source":["# Defining a train function to train the model such that for every 10 epochs the model will return respective weights \n","def train(text, epochs=100, save_freq=10):\n","\n","    #model_architecture\n","    model = build_model(batch_size, batch_sequence, vocab_size)\n","    print(model.summary())\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","\n","    #Train data generation\n","    T = np.asarray([char_to_idx[c] for c in text], dtype=np.int32) #convert complete text into numerical indices\n","\n","    print(\"Length of text:\" + str(T.size)) #129,665\n","\n","    steps_per_epoch = (len(text) / batch_size - 1) / batch_sequence  \n","\n","    for epoch in range(epochs):\n","        print('\\nEpoch {}/{}'.format(epoch + 1, epochs))\n","        losses, accs = [], []\n","        for i, (X, Y) in enumerate(generate_batchs(T, vocab_size)):\n","            loss, acc = model.train_on_batch(X, Y)\n","            losses.append(loss)\n","            accs.append(acc)\n","        print('epoch {}: loss = {}, acc = {}'.format(epoch + 1, np.mean(loss), np.mean(acc)))\n","        \n","\n","        if (epoch + 1) % 10 == 0:\n","            save_weights(epoch + 1, model)\n","            print('Saved checkpoint to', 'weights.{}.h5'.format(epoch + 1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y68qFdoOCFQY","outputId":"f8a75db0-88e3-44e1-b2d4-1a7ce0e5f253"},"source":["train(input_text)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (16, 64, 512)             44032     \n","                                                                 \n"," lstm (LSTM)                 (16, 64, 256)             787456    \n","                                                                 \n"," dropout (Dropout)           (16, 64, 256)             0         \n","                                                                 \n"," lstm_1 (LSTM)               (16, 64, 256)             525312    \n","                                                                 \n"," dropout_1 (Dropout)         (16, 64, 256)             0         \n","                                                                 \n"," lstm_2 (LSTM)               (16, 64, 256)             525312    \n","                                                                 \n"," dropout_2 (Dropout)         (16, 64, 256)             0         \n","                                                                 \n"," time_distributed (TimeDistr  (16, 64, 86)             22102     \n"," ibuted)                                                         \n","                                                                 \n"," activation (Activation)     (16, 64, 86)              0         \n","                                                                 \n","=================================================================\n","Total params: 1,904,214\n","Trainable params: 1,904,214\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Length of text:129665\n","\n","Epoch 1/100\n","epoch 1: loss = 2.9406843185424805, acc = 0.193359375\n","\n","Epoch 2/100\n","epoch 2: loss = 2.151693820953369, acc = 0.3935546875\n","\n","Epoch 3/100\n","epoch 3: loss = 1.839420199394226, acc = 0.4912109375\n","\n","Epoch 4/100\n","epoch 4: loss = 1.703432321548462, acc = 0.513671875\n","\n","Epoch 5/100\n","epoch 5: loss = 1.5986555814743042, acc = 0.525390625\n","\n","Epoch 6/100\n","epoch 6: loss = 1.5212668180465698, acc = 0.552734375\n","\n","Epoch 7/100\n","epoch 7: loss = 1.4740604162216187, acc = 0.5634765625\n","\n","Epoch 8/100\n","epoch 8: loss = 1.4189280271530151, acc = 0.57421875\n","\n","Epoch 9/100\n","epoch 9: loss = 1.3598912954330444, acc = 0.5908203125\n","\n","Epoch 10/100\n","epoch 10: loss = 1.318968415260315, acc = 0.5888671875\n","Saved checkpoint to weights.10.h5\n","\n","Epoch 11/100\n","epoch 11: loss = 1.2729241847991943, acc = 0.591796875\n","\n","Epoch 12/100\n","epoch 12: loss = 1.2662701606750488, acc = 0.607421875\n","\n","Epoch 13/100\n","epoch 13: loss = 1.2277066707611084, acc = 0.619140625\n","\n","Epoch 14/100\n","epoch 14: loss = 1.171022891998291, acc = 0.640625\n","\n","Epoch 15/100\n","epoch 15: loss = 1.1570085287094116, acc = 0.625\n","\n","Epoch 16/100\n","epoch 16: loss = 1.1284407377243042, acc = 0.6376953125\n","\n","Epoch 17/100\n","epoch 17: loss = 1.0876682996749878, acc = 0.6513671875\n","\n","Epoch 18/100\n","epoch 18: loss = 1.0900332927703857, acc = 0.65234375\n","\n","Epoch 19/100\n","epoch 19: loss = 1.0222673416137695, acc = 0.6708984375\n","\n","Epoch 20/100\n","epoch 20: loss = 1.0094228982925415, acc = 0.681640625\n","Saved checkpoint to weights.20.h5\n","\n","Epoch 21/100\n","epoch 21: loss = 0.9903829097747803, acc = 0.6826171875\n","\n","Epoch 22/100\n","epoch 22: loss = 0.9551690220832825, acc = 0.69921875\n","\n","Epoch 23/100\n","epoch 23: loss = 0.9385437965393066, acc = 0.7001953125\n","\n","Epoch 24/100\n","epoch 24: loss = 0.9254482388496399, acc = 0.7109375\n","\n","Epoch 25/100\n","epoch 25: loss = 0.8950438499450684, acc = 0.7197265625\n","\n","Epoch 26/100\n","epoch 26: loss = 0.8683862686157227, acc = 0.7333984375\n","\n","Epoch 27/100\n","epoch 27: loss = 0.8778054714202881, acc = 0.728515625\n","\n","Epoch 28/100\n","epoch 28: loss = 0.8153538107872009, acc = 0.7587890625\n","\n","Epoch 29/100\n","epoch 29: loss = 0.8157223463058472, acc = 0.7333984375\n","\n","Epoch 30/100\n","epoch 30: loss = 0.7973155379295349, acc = 0.7392578125\n","Saved checkpoint to weights.30.h5\n","\n","Epoch 31/100\n","epoch 31: loss = 0.7640082836151123, acc = 0.7548828125\n","\n","Epoch 32/100\n","epoch 32: loss = 0.78264319896698, acc = 0.763671875\n","\n","Epoch 33/100\n","epoch 33: loss = 0.7333216667175293, acc = 0.76953125\n","\n","Epoch 34/100\n","epoch 34: loss = 0.7018999457359314, acc = 0.765625\n","\n","Epoch 35/100\n","epoch 35: loss = 0.7053638696670532, acc = 0.775390625\n","\n","Epoch 36/100\n","epoch 36: loss = 0.6757709383964539, acc = 0.7822265625\n","\n","Epoch 37/100\n","epoch 37: loss = 0.6531789302825928, acc = 0.7744140625\n","\n","Epoch 38/100\n","epoch 38: loss = 0.6780627965927124, acc = 0.78125\n","\n","Epoch 39/100\n","epoch 39: loss = 0.641050398349762, acc = 0.77734375\n","\n","Epoch 40/100\n","epoch 40: loss = 0.5974858999252319, acc = 0.8046875\n","Saved checkpoint to weights.40.h5\n","\n","Epoch 41/100\n","epoch 41: loss = 0.6288197636604309, acc = 0.8037109375\n","\n","Epoch 42/100\n","epoch 42: loss = 0.5691646337509155, acc = 0.8193359375\n","\n","Epoch 43/100\n","epoch 43: loss = 0.5726351737976074, acc = 0.80859375\n","\n","Epoch 44/100\n","epoch 44: loss = 0.5445736646652222, acc = 0.80859375\n","\n","Epoch 45/100\n","epoch 45: loss = 0.5745640993118286, acc = 0.8115234375\n","\n","Epoch 46/100\n","epoch 46: loss = 0.5235417485237122, acc = 0.8212890625\n","\n","Epoch 47/100\n","epoch 47: loss = 0.5057986974716187, acc = 0.8310546875\n","\n","Epoch 48/100\n","epoch 48: loss = 0.49633556604385376, acc = 0.83203125\n","\n","Epoch 49/100\n","epoch 49: loss = 0.4984433650970459, acc = 0.84765625\n","\n","Epoch 50/100\n","epoch 50: loss = 0.47016480565071106, acc = 0.845703125\n","Saved checkpoint to weights.50.h5\n","\n","Epoch 51/100\n","epoch 51: loss = 0.4742494523525238, acc = 0.8369140625\n","\n","Epoch 52/100\n","epoch 52: loss = 0.45069295167922974, acc = 0.849609375\n","\n","Epoch 53/100\n","epoch 53: loss = 0.4735579490661621, acc = 0.82421875\n","\n","Epoch 54/100\n","epoch 54: loss = 0.44676393270492554, acc = 0.8486328125\n","\n","Epoch 55/100\n","epoch 55: loss = 0.4390561580657959, acc = 0.8583984375\n","\n","Epoch 56/100\n","epoch 56: loss = 0.41795286536216736, acc = 0.865234375\n","\n","Epoch 57/100\n","epoch 57: loss = 0.4049769937992096, acc = 0.8603515625\n","\n","Epoch 58/100\n","epoch 58: loss = 0.4178779125213623, acc = 0.8564453125\n","\n","Epoch 59/100\n","epoch 59: loss = 0.4128088653087616, acc = 0.8564453125\n","\n","Epoch 60/100\n","epoch 60: loss = 0.38267165422439575, acc = 0.8603515625\n","Saved checkpoint to weights.60.h5\n","\n","Epoch 61/100\n","epoch 61: loss = 0.40297597646713257, acc = 0.8642578125\n","\n","Epoch 62/100\n","epoch 62: loss = 0.36722445487976074, acc = 0.8798828125\n","\n","Epoch 63/100\n","epoch 63: loss = 0.38277938961982727, acc = 0.873046875\n","\n","Epoch 64/100\n","epoch 64: loss = 0.38230857253074646, acc = 0.875\n","\n","Epoch 65/100\n","epoch 65: loss = 0.35182109475135803, acc = 0.8876953125\n","\n","Epoch 66/100\n","epoch 66: loss = 0.32952868938446045, acc = 0.884765625\n","\n","Epoch 67/100\n","epoch 67: loss = 0.3590852916240692, acc = 0.8701171875\n","\n","Epoch 68/100\n","epoch 68: loss = 0.33536964654922485, acc = 0.884765625\n","\n","Epoch 69/100\n","epoch 69: loss = 0.3284801244735718, acc = 0.890625\n","\n","Epoch 70/100\n","epoch 70: loss = 0.29769283533096313, acc = 0.9013671875\n","Saved checkpoint to weights.70.h5\n","\n","Epoch 71/100\n","epoch 71: loss = 0.3476465940475464, acc = 0.8798828125\n","\n","Epoch 72/100\n","epoch 72: loss = 0.3324176073074341, acc = 0.8994140625\n","\n","Epoch 73/100\n","epoch 73: loss = 0.34226444363594055, acc = 0.8828125\n","\n","Epoch 74/100\n","epoch 74: loss = 0.31539595127105713, acc = 0.8896484375\n","\n","Epoch 75/100\n","epoch 75: loss = 0.34258753061294556, acc = 0.88671875\n","\n","Epoch 76/100\n","epoch 76: loss = 0.3048223853111267, acc = 0.9013671875\n","\n","Epoch 77/100\n","epoch 77: loss = 0.3406526446342468, acc = 0.88671875\n","\n","Epoch 78/100\n","epoch 78: loss = 0.31828412413597107, acc = 0.900390625\n","\n","Epoch 79/100\n","epoch 79: loss = 0.27648571133613586, acc = 0.912109375\n","\n","Epoch 80/100\n","epoch 80: loss = 0.2871294915676117, acc = 0.8896484375\n","Saved checkpoint to weights.80.h5\n","\n","Epoch 81/100\n","epoch 81: loss = 0.26168292760849, acc = 0.9111328125\n","\n","Epoch 82/100\n","epoch 82: loss = 0.3219965994358063, acc = 0.884765625\n","\n","Epoch 83/100\n","epoch 83: loss = 0.24393126368522644, acc = 0.921875\n","\n","Epoch 84/100\n","epoch 84: loss = 0.26464104652404785, acc = 0.9189453125\n","\n","Epoch 85/100\n","epoch 85: loss = 0.2664584517478943, acc = 0.912109375\n","\n","Epoch 86/100\n","epoch 86: loss = 0.2690492570400238, acc = 0.904296875\n","\n","Epoch 87/100\n","epoch 87: loss = 0.24706491827964783, acc = 0.916015625\n","\n","Epoch 88/100\n","epoch 88: loss = 0.2655051350593567, acc = 0.9111328125\n","\n","Epoch 89/100\n","epoch 89: loss = 0.26696670055389404, acc = 0.91015625\n","\n","Epoch 90/100\n","epoch 90: loss = 0.2301967442035675, acc = 0.9208984375\n","Saved checkpoint to weights.90.h5\n","\n","Epoch 91/100\n","epoch 91: loss = 0.2594178020954132, acc = 0.90625\n","\n","Epoch 92/100\n","epoch 92: loss = 0.2771235406398773, acc = 0.9091796875\n","\n","Epoch 93/100\n","epoch 93: loss = 0.2791612148284912, acc = 0.9169921875\n","\n","Epoch 94/100\n","epoch 94: loss = 0.23604032397270203, acc = 0.9111328125\n","\n","Epoch 95/100\n","epoch 95: loss = 0.251932293176651, acc = 0.9189453125\n","\n","Epoch 96/100\n","epoch 96: loss = 0.2348681092262268, acc = 0.9169921875\n","\n","Epoch 97/100\n","epoch 97: loss = 0.2476586103439331, acc = 0.9228515625\n","\n","Epoch 98/100\n","epoch 98: loss = 0.22246727347373962, acc = 0.927734375\n","\n","Epoch 99/100\n","epoch 99: loss = 0.23673541843891144, acc = 0.919921875\n","\n","Epoch 100/100\n","epoch 100: loss = 0.21710890531539917, acc = 0.9267578125\n","Saved checkpoint to weights.100.h5\n"]}]},{"cell_type":"markdown","metadata":{"id":"J7Y1iNSs8evE"},"source":["----\n","<a id=section7></a>\n","# **7. Music Genaration**\n","----\n","- In this step we will genarate the **abc notes**.\n","- Later on these notes will be put <a href=\"https://www.abcjs.net/abcjs-editor.html\">here</a> \n","- Then we will be able to hear the **respective notes.**"]},{"cell_type":"code","metadata":{"id":"Qa1q9w6EIY1B"},"source":["# Setting the data and the model directories respectively\n","DATA_DIR = '/content/'\n","MODEL_DIR = '/content/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6FzJ7sAkIlJp"},"source":["# Save the weights into the respective model directory path with respective model names\n","def save_weights(epoch, model):\n","    if not os.path.exists(MODEL_DIR):\n","        os.makedirs(MODEL_DIR)\n","    model.save_weights(os.path.join(MODEL_DIR, 'weights.{}.h5'.format(epoch)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vKUYFEgGYcxR"},"source":["# Loading the weights into the model\n","def load_weights(epoch, model):\n","    model.load_weights(os.path.join(MODEL_DIR, 'weights.{}.h5'.format(epoch)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9VPIHq7WInXs"},"source":["# Building the test model with the given vocab size = 86 \n","def build_test_model(vocab_size):\n","    model = Sequential()\n","    model.add(Embedding(vocab_size,\n","                        512,\n","                        batch_input_shape=(1,1)))\n","    for i in range(3):\n","        model.add(LSTM(256,\n","                       return_sequences=(i != 2),\n","                       stateful=True))\n","        model.add(Dropout(0.2))\n","\n","    model.add(Dense(vocab_size))\n","    model.add(Activation('softmax'))\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l2AlyizAIrwl"},"source":["# Genarating the text corpus upon the test model\n","def generate_music(epoch, num_char):\n","    with open(os.path.join(DATA_DIR, 'char_to_idx')) as f:\n","        char_to_idx = json.load(f)\n","\n","    idx_to_char = dict(zip(char_to_idx.values(), char_to_idx.keys()))\n","\n","    # Feeding the pre-defined parameters\n","    vocab_size = len(char_to_idx)\n","    batch_size = 16\n","    seq_len = 64\n","\n","    # calling the test model with vocab size = 86\n","    model =  build_test_model(vocab_size)\n","    load_weights(epoch, model)\n","\n","    sampled = []\n","    batch = np.zeros((1, 1))\n","    batch[0, 0] = np.random.randint(vocab_size)\n","    for i in range(num_char):\n","        if i ==0:\n","            #Predicting for the respective batches and returning a flatten array\n","            result = model.predict_on_batch(batch).ravel()\n","            sample = np.random.choice(range(vocab_size), p=result)\n","            sampled.append(idx_to_char[sample])\n","        else:\n","            #Reassigning the batch size as of the sample size\n","            batch[0,0] = np.array([[sample]])\n","            \n","            #Predicting for the respective batches and returning a flatten array\n","            result = model.predict_on_batch(batch).ravel()\n","            sample = np.random.choice(range(vocab_size), p=result)\n","            sampled.append(idx_to_char[sample])\n","    return ''.join(sampled)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"lrQaDmzDJ7Uf","outputId":"2e1b9da3-7da4-4213-d2ac-2d020ac45906"},"source":["generate_music(10, 512)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'?fef dAG|\"D\"FEF \"C\"D2:|[2 \"Dm\"A2E FGG|\\\\\\n\"D\"G3 cBG|\"C\"FA2d eec|\"D\"a3 fc:|\\n\\n\\nX: 1U6\\nT:PTee Cher\\nogt!ers\\n% Nottingham Music Datebase\\nS:TKl, via EF\\nY:AB\\nM:6/8\\nK:G\\nP:D\\nA/2A/2|\"D\"c2c d2c|\"A\"dee \"D\"ded|\"D\"ddd \"G\"dBG|\"G\"BGG G2B|\\n\"Am\"fdd \"G\"Bcc|\"D\"A2c bdf|\"A\"edc \"E\"Ade|\"G\"d3 d2:|\\nP:B\\nf|\"G\"gba e2d|c^G Bcd|\"Dm\"cdd e2d|\"D\"c2A ABc|\\n\"D\"ddd Aiaf|\"G\"ggg feg|\"A\"e2c \"A\"\"G7\"A2c| \"G\"g2d a2g|\"D\"f2f a^ga|\\n\"A7\"g3+b2fB/2f/2g geb|\"G\"g3 -gdd|\"BAAB/A\"A7/B#7\"G2B|\"D\"dec \"A7\"c3\\n|\\n\\n\\nX: 283\\nT:Jory\\'d Bancle\\n% Nottingham Music Database\\nS:Ch'"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"gCcpV7Xm5gW0"},"source":["### **Observation:**\n","\n","- This is the rendered output of the **input text corpus** that was given to the model.\n","\n","- Now, this output is of **ABC Notation of music**. You can learn more about **ABC Notation** <a href=\"https://en.wikipedia.org/wiki/ABC_notation\">here!</a>\n","\n","- This output we will copy and paste <a href=\"https://www.abcjs.net/abcjs-editor.html\">here!</a> and press the **Play button**\n"]},{"cell_type":"markdown","metadata":{"id":"jcq2hpAsSoSi"},"source":["----\n","<a id=section8></a>\n","# **8. Applications**\n","----\n","\n","- This model can be used for in-house **music production systems.**\n","- This can be widely used to automate **manual instruments.**\n","- This can be also used to make **automatic VST(virtual studio toolkit) plugins**"]},{"cell_type":"markdown","metadata":{"id":"dRSF1QafSrUo"},"source":["----\n","<a id=section9></a>\n","# **9. Limitations**\n","----\n","- The only limitation of this model is that it is being trained with **very less data.**\n","\n","- However, while getting trained on various intrument data this model can be further enhanced for **different instruments** as well.\n","\n","- We have **trained this model for only 100 epochs**. As the **number of epochs increase** it is expected that the **accuracy of the model will increase**."]},{"cell_type":"markdown","metadata":{"id":"-EkLTGCbSvsm"},"source":["----\n","<a id=section10></a>\n","# **10. Conclusion**\n","----\n","\n","- In this project an **Automatic music genaration system** was made from scratch.\n","- Here, we recieved a **validation accuracy of 92%**.\n","- This project can be widely used for **music production systems**\n","- The only limitation of this model is that it is being trained with **very less data.**\n","- However, while getting trained on various intrument data this model can be further enhanced for different instruments as well.\n"]}]}